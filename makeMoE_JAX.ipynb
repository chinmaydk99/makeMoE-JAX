{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import jax.random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
    "    n_embd : int\n",
    "    dropout_rate : float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training : bool = True):\n",
    "        x = nn.Dense(4 * self.n_embd, name = \"w1\")(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.n_embd, name = \"w2\")(x)\n",
    "        x = nn.Dropout(\n",
    "            rate = self.dropout_rate,\n",
    "            deterministic = not training\n",
    "        )(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRouter(nn.Module):\n",
    "    \"\"\" Router that selects top-k experts for each token \"\"\"\n",
    "    n_embd : int\n",
    "    num_experts : int\n",
    "    top_k : int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        logits = nn.Dense(self.num_experts)(x) # (batch, seq_len, num_experts)\n",
    "        top_k_logits, top_k_indices = jax.lax.top_k(logits, self.top_k)\n",
    "        zeros = jnp.full_like(logits, -jnp.inf)\n",
    "        sparse_logits = jnp.put_along_axis(zeros, top_k_indices, top_k_logits, axis = -1, inplace = False)\n",
    "        router_output = jax.nn.softmax(sparse_logits, axis = -1)\n",
    "        return router_output, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTopKRouter(nn.Module):\n",
    "    \"\"\" Router that selects top-k experts for each token with noise \"\"\"\n",
    "    n_embd : int\n",
    "    num_experts : int\n",
    "    top_k : int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, rng_key):\n",
    "        logits = nn.Dense(self.num_experts)(x) # (batch, seq_len, num_experts)\n",
    "        noise_logits = nn.Dense(self.num_experts)(x) # (batch, seq_len, num_experts)\n",
    "\n",
    "        noise = jax.random.normal(rng_key, logits.shape)\n",
    "        noise = noise * jax.nn.softplus(noise_logits)\n",
    "        logits = logits + noise\n",
    "\n",
    "        top_k_logits, top_k_indices = jax.lax.top_k(logits, self.top_k)\n",
    "        zeros = jnp.full_like(logits, -jnp.inf)\n",
    "        sparse_logits = jnp.put_along_axis(zeros, top_k_indices, top_k_logits, axis = -1, inplace = False)\n",
    "        router_output = jax.nn.softmax(sparse_logits, axis = -1)\n",
    "        return router_output, top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse MoE Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    \"\"\"Sparse Mixture of Experts using JAX's batching capabilities\"\"\"\n",
    "    n_embd: int\n",
    "    num_experts: int\n",
    "    top_k: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.router = NoisyTopKRouter(\n",
    "            n_embd=self.n_embd,\n",
    "            num_experts=self.num_experts,\n",
    "            top_k=self.top_k\n",
    "        )\n",
    "        self.experts = [Expert(n_embd=self.n_embd) for _ in range(self.num_experts)]\n",
    "    \n",
    "    def __call__(self, x, training=False, rng_key=None):\n",
    "        # Get shape of input\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Get routing probabilities and expert indices\n",
    "        router_weights, router_indices = self.router(x, rng_key=rng_key) # ( batch, seq_len, num_experts)\n",
    "        \n",
    "        # Initialize the output tensor upfront - don't check if it exists later\n",
    "        final_output = jnp.zeros_like(x) # (batch, seq_len, n_embd)\n",
    "        \n",
    "        # Flattening the input and router weights and indices\n",
    "        x_reshaped = x.reshape(-1, C) # (batch * seq_len, n_embd)\n",
    "        router_weights_flat = router_weights.reshape(-1, self.num_experts) # (batch * seq_len, num_experts)\n",
    "        router_indices_flat = router_indices.reshape(-1, self.top_k) # (batch * seq_len, top_k)\n",
    "        \n",
    "        # Process each expert\n",
    "        for e in range(self.num_experts):\n",
    "            # Create mask for tokens going to this expert\n",
    "            expert_mask = (router_indices == e).any(axis=-1) # (batch, seq_len)\n",
    "            \n",
    "            # Skip if no tokens assigned to this expert\n",
    "            if not jnp.any(expert_mask):\n",
    "                continue\n",
    "                \n",
    "            # Get positions where this expert is used\n",
    "            token_indices = jnp.where(expert_mask) # (num_tokens, 2)\n",
    "            batch_idx, seq_idx = token_indices\n",
    "            \n",
    "            # Flatten indices for easier extraction\n",
    "            # Converts (batch_idx, seq_idx) to a 1D index\n",
    "            flat_indices = batch_idx * T + seq_idx # (num_tokens)\n",
    "            \n",
    "            # Extract inputs for this expert\n",
    "             # Dispatch step : Only gathering tokens that need to be processed by this expert\n",
    "            expert_inputs = x_reshaped[flat_indices] # (num_tokens, n_embd)\n",
    "            \n",
    "            # Process through the expert\n",
    "            expert_outputs = self.experts[e](expert_inputs, training=training) # (num_tokens, n_embd)\n",
    "            \n",
    "            # Get corresponding weights\n",
    "            expert_weights = router_weights[batch_idx, seq_idx, e]\n",
    "            \n",
    "            # Weight the outputs\n",
    "            weighted_outputs = expert_outputs * expert_weights[:, None]\n",
    "            \n",
    "            # Use functional update to add to final output\n",
    "            for i, (b_idx, s_idx) in enumerate(zip(batch_idx, seq_idx)):\n",
    "                final_output = final_output.at[b_idx, s_idx].add(weighted_outputs[i])\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_moe_components():\n",
    "#     \"\"\"Test each component of the MoE implementation individually\"\"\"\n",
    "#     print(\"Testing MoE components...\")\n",
    "    \n",
    "#     # Set up a consistent RNG key\n",
    "#     main_key = random.PRNGKey(42)\n",
    "    \n",
    "#     # Test parameters - small values for quick testing\n",
    "#     batch_size = 4\n",
    "#     seq_len = 8\n",
    "#     embed_dim = 16\n",
    "#     num_experts = 4\n",
    "#     top_k = 2\n",
    "    \n",
    "#     # Create dummy input data\n",
    "#     main_key, input_key = random.split(main_key)\n",
    "#     dummy_tokens = random.randint(input_key, (batch_size, seq_len), 0, 100)\n",
    "#     dummy_embed = random.normal(input_key, (batch_size, seq_len, embed_dim))\n",
    "    \n",
    "    \n",
    "#     # 3. Test the Expert component\n",
    "#     print(\"\\n3. Testing Expert component...\")\n",
    "#     class ExpertTest(nn.Module):\n",
    "#         @nn.compact\n",
    "#         def __call__(self, x, training=False):\n",
    "#             expert = Expert(n_embd=embed_dim)\n",
    "#             return expert(x, training=training)\n",
    "    \n",
    "#     expert_model = ExpertTest()\n",
    "#     main_key, expert_key = random.split(main_key)\n",
    "#     expert_params = expert_model.init(expert_key, dummy_embed)\n",
    "#     expert_output = expert_model.apply(expert_params, dummy_embed)\n",
    "#     print(f\"  Expert output shape: {expert_output.shape} (expected: {(batch_size, seq_len, embed_dim)})\")\n",
    "    \n",
    "#     # 4. Test the NoisyTopkRouter component\n",
    "#     print(\"\\n4. Testing NoisyTopkRouter component...\")\n",
    "#     class RouterTest(nn.Module):\n",
    "#         @nn.compact\n",
    "#         def __call__(self, x, training=False, rng_key=None):\n",
    "#             router = NoisyTopKRouter(n_embd=embed_dim, num_experts=num_experts, top_k=top_k)\n",
    "#             return router(x, rng_key=rng_key)\n",
    "    \n",
    "#     router_model = RouterTest()\n",
    "#     main_key, router_key, router_rng = random.split(main_key, 3)\n",
    "#     router_params = router_model.init({'params': router_key, 'dropout': router_rng}, \n",
    "#                                       dummy_embed, training=True, rng_key=router_rng)\n",
    "#     # Properly handle the return value when using mutable\n",
    "#     outputs = router_model.apply(\n",
    "#         router_params, dummy_embed, training=True, rng_key=router_rng, \n",
    "#         mutable=['intermediates'], rngs={'dropout': router_rng}\n",
    "#     )\n",
    "#     # When using mutable, the return value is (outputs, mutated_vars)\n",
    "#     outputs, _ = outputs  # Unpack the tuple\n",
    "#     router_output, indices = outputs  # Now unpack the actual outputs\n",
    "    \n",
    "#     print(f\"  Router output shape: {router_output.shape} (expected: {(batch_size, seq_len, num_experts)})\")\n",
    "#     print(f\"  Router indices shape: {indices.shape} (expected: {(batch_size, seq_len, top_k)})\")\n",
    "    \n",
    "#     # 5. Test the SparseMoE component\n",
    "#     print(\"\\n5. Testing SparseMoE component...\")\n",
    "#     class MoETest(nn.Module):\n",
    "#         @nn.compact\n",
    "#         def __call__(self, x, training=False, rng_key=None):\n",
    "#             moe = SparseMoE(n_embd=embed_dim, num_experts=num_experts, top_k=top_k)\n",
    "#             return moe(x, training=training, rng_key=rng_key)\n",
    "    \n",
    "#     moe_model = MoETest()\n",
    "#     main_key, moe_key, moe_rng = random.split(main_key, 3)\n",
    "#     moe_params = moe_model.init({'params': moe_key, 'dropout': moe_rng}, \n",
    "#                                dummy_embed, training=True, rng_key=moe_rng)\n",
    "#     # Properly handle the return value when using mutable\n",
    "#     outputs = moe_model.apply(\n",
    "#         moe_params, dummy_embed, training=True, rng_key=moe_rng,\n",
    "#         mutable=['intermediates'], rngs={'dropout': moe_rng}\n",
    "#     )\n",
    "#     moe_output, _ = outputs  # Unpack the tuple\n",
    "    \n",
    "#     print(f\"  MoE output shape: {moe_output.shape} (expected: {(batch_size, seq_len, embed_dim)})\")\n",
    "\n",
    "# test_moe_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    head_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training : bool = False):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Create key, query, and value projections\n",
    "        key = nn.Dense(self.head_size, use_bias=False, name =\"key\")(x)\n",
    "        query = nn.Dense(self.head_size, use_bias=False, name =\"query\")(x)\n",
    "        value = nn.Dense(self.head_size, use_bias=False, name =\"value\")(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = query @ jnp.swapaxes(key, -2, -1) * (self.head_size**-0.5)\n",
    "        \n",
    "        # Create causal mask for the current block\n",
    "        T_use = min(T, self.block_size)\n",
    "        mask = jnp.tril(jnp.ones((T_use, T_use)))\n",
    "        mask = jnp.broadcast_to(mask, (x.shape[0], T_use, T_use)) # (T_use, T_use) -> (B, T_use, T_use)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        wei = jnp.where(mask, wei, -jnp.inf)\n",
    "\n",
    "        wei = jax.nn.softmax(wei, axis = -1)\n",
    "        wei = nn.Dropout(self.dropout_rate, deterministic = not training)(wei)\n",
    "\n",
    "        out = wei @ value\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "    n_embd: int\n",
    "    n_head: int\n",
    "    head_size: int\n",
    "    block_size: int\n",
    "    dropout_rate: float = 0.1\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, training=False):\n",
    "        # Create multiple heads\n",
    "        heads = [Head(\n",
    "            head_size=self.head_size,\n",
    "            n_embd=self.n_embd,\n",
    "            block_size=self.block_size,\n",
    "            dropout_rate=self.dropout_rate\n",
    "        ) for _ in range(self.n_head)]\n",
    "        \n",
    "        # Apply each head and concatenate results\n",
    "        head_outputs = [head(x, training=training) for head in heads]\n",
    "        out = jnp.concatenate(head_outputs, axis=-1)\n",
    "        \n",
    "        # Project back to embedding dimension\n",
    "        out = nn.Dense(self.n_embd)(out)\n",
    "        out = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_attention_components():\n",
    "#     \"\"\"Test the attention components\"\"\"\n",
    "#     print(\"Testing attention components...\")\n",
    "    \n",
    "#     # Set up a consistent RNG key\n",
    "#     main_key = jax.random.PRNGKey(42)\n",
    "    \n",
    "#     # Test parameters\n",
    "#     batch_size = 4\n",
    "#     seq_len = 8\n",
    "#     embed_dim = 32\n",
    "#     n_head = 4\n",
    "#     head_size = embed_dim // n_head\n",
    "#     block_size = 32\n",
    "    \n",
    "#     # Create dummy input\n",
    "#     main_key, input_key = jax.random.split(main_key)\n",
    "#     dummy_embed = jax.random.normal(input_key, (batch_size, seq_len, embed_dim))\n",
    "    \n",
    "#     # Test Head\n",
    "#     print(\"\\nTesting Head...\")\n",
    "#     class HeadTest(nn.Module):\n",
    "#         @nn.compact\n",
    "#         def __call__(self, x, training=False):\n",
    "#             head = Head(head_size=head_size, n_embd=embed_dim, block_size=block_size)\n",
    "#             return head(x, training=training)\n",
    "    \n",
    "#     head_model = HeadTest()\n",
    "#     main_key, head_key = jax.random.split(main_key)\n",
    "#     head_params = head_model.init({'params': head_key}, dummy_embed)\n",
    "#     head_output = head_model.apply(head_params, dummy_embed)\n",
    "#     print(f\"  Head output shape: {head_output.shape} (expected: {(batch_size, seq_len, head_size)})\")\n",
    "    \n",
    "#     # Test MultiHeadAttention\n",
    "#     print(\"\\nTesting MultiHeadAttention...\")\n",
    "#     class MHATest(nn.Module):\n",
    "#         @nn.compact\n",
    "#         def __call__(self, x, training=False):\n",
    "#             mha = MultiHeadAttention(\n",
    "#                 n_embd=embed_dim, \n",
    "#                 n_head=n_head, \n",
    "#                 head_size=head_size, \n",
    "#                 block_size=block_size\n",
    "#             )\n",
    "#             return mha(x, training=training)\n",
    "    \n",
    "#     mha_model = MHATest()\n",
    "#     main_key, mha_key = jax.random.split(main_key)\n",
    "#     mha_params = mha_model.init({'params': mha_key}, dummy_embed)\n",
    "    \n",
    "#     # Use rngs parameter to handle dropout properly\n",
    "#     mha_output = mha_model.apply(\n",
    "#         mha_params, \n",
    "#         dummy_embed, \n",
    "#         training=True,\n",
    "#         rngs={'dropout': mha_key}\n",
    "#     )\n",
    "#     print(f\"  MultiHeadAttention output shape: {mha_output.shape} (expected: {(batch_size, seq_len, embed_dim)})\")\n",
    "    \n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing attention components...\n",
      "\n",
      "Testing Head...\n",
      "  Head output shape: (4, 8, 8) (expected: (4, 8, 8))\n",
      "\n",
      "Testing MultiHeadAttention...\n",
      "  MultiHeadAttention output shape: (4, 8, 32) (expected: (4, 8, 32))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_attention_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    n_embd : int\n",
    "    n_head : int\n",
    "    num_experts : int\n",
    "    block_size : int\n",
    "    top_k : int\n",
    "\n",
    "    def setup(self):\n",
    "        self.sa = MultiHeadAttention(\n",
    "            n_embd = self.n_embd,\n",
    "            n_head = self.n_head,\n",
    "            head_size = self.n_embd // self.n_head,\n",
    "            block_size = self.block_size)\n",
    "        \n",
    "        self.smoe = SparseMoE(\n",
    "            n_embd = self.n_embd,\n",
    "            num_experts = self.num_experts,\n",
    "            top_k = self.top_k\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm()\n",
    "        self.ln2 = nn.LayerNorm()\n",
    "    \n",
    "    def __call__(self, x, rng_key, training : bool = False):\n",
    "        sa_key, moe_key = jax.random.split(rng_key)\n",
    "\n",
    "        # Self attention with residual connection\n",
    "        x = x + self.sa(self.ln1(x), training = training)\n",
    "\n",
    "        # Sparse MoE with residual connection\n",
    "        x = x + self.smoe(self.ln2(x), training = training, rng_key = moe_key)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_block():\n",
    "#     \"\"\"Test the transformer block\"\"\"\n",
    "#     print(\"Testing Block...\")\n",
    "    \n",
    "#     # Set up a consistent RNG key\n",
    "#     main_key = jax.random.PRNGKey(42)\n",
    "    \n",
    "#     # Test parameters\n",
    "#     batch_size = 4\n",
    "#     seq_len = 8\n",
    "#     embed_dim = 32\n",
    "#     n_head = 4\n",
    "#     num_experts = 4\n",
    "#     top_k = 2\n",
    "#     block_size = 32\n",
    "    \n",
    "#     # Create dummy input\n",
    "#     main_key, input_key = jax.random.split(main_key)\n",
    "#     dummy_embed = jax.random.normal(input_key, (batch_size, seq_len, embed_dim))\n",
    "    \n",
    "#     # Test Block\n",
    "#     class BlockTest(nn.Module):\n",
    "#         @nn.compact\n",
    "#         def __call__(self, x, training=False, rng_key=None):\n",
    "#             block = Block(\n",
    "#                 n_embd=embed_dim,\n",
    "#                 n_head=n_head,\n",
    "#                 num_experts=num_experts,\n",
    "#                 block_size=block_size,\n",
    "#                 top_k=top_k\n",
    "#             )\n",
    "#             return block(x, rng_key=rng_key, training=training)\n",
    "    \n",
    "#     block_model = BlockTest()\n",
    "#     main_key, block_key, block_rng = jax.random.split(main_key, 3)\n",
    "    \n",
    "#     # Initialize the model with the correct RNG keys\n",
    "#     block_params = block_model.init(\n",
    "#         {'params': block_key, 'dropout': block_rng}, \n",
    "#         dummy_embed, training=True, rng_key=block_rng\n",
    "#     )\n",
    "    \n",
    "#     # Apply the block\n",
    "#     main_key, apply_rng = jax.random.split(main_key)\n",
    "#     block_output = block_model.apply(\n",
    "#         block_params, \n",
    "#         dummy_embed, \n",
    "#         training=True, \n",
    "#         rng_key=apply_rng,\n",
    "#         rngs={'dropout': apply_rng}\n",
    "#     )\n",
    "    \n",
    "#     print(f\"  Block output shape: {block_output.shape} (expected: {(batch_size, seq_len, embed_dim)})\")\n",
    "    \n",
    "#     return True\n",
    "\n",
    "# test_block()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
